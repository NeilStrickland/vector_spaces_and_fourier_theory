\begin{document}

\opengraphsfile{pics/pics04}

\begin{center}
 {\huge Vector Spaces and Fourier Theory ---
   Problem Sheet 4
 }
\end{center}

\begin{rubric}
 There is an online test this week, which covers most of the
 questions on this sheet.
\end{rubric}
\begin{exercise}\label{ex-check-independence}
 Which of the following lists of vectors are linearly
 independent?\\[0.1ex]
 \begin{itemize}\renewcommand{\itemsep}{1ex}
  \item[(a)]
   $\vu_1=[1,0,0,0,1]^T,\vu_2=[0,2,0,2,0]^T,\vu_3=[0,0,3,0,0]^T$
  \item[(b)]
   $\vv_1=[1,1,1,1]^T,\vv_2=[2,0,0,2]^T,\vv_3=[0,4,4,0]^T$
  \item[(c)]
   $\vw_1=[1,1,2]^T,\vw_2=[4,5,7]^T,\vw_3=[1,1,1]^T$ 
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] These vectors are linearly independent.
   Indeed, we have 
   \[ \lm_1\vu_1+\lm_2\vu_2+\lm_3\vu_3 =
       [\lm_1,2\lm_2,3\lm_3,2\lm_2,\lm_1]^T, 
   \]
   and this can only be zero if $\lm_1=\lm_2=\lm_3=0$.
   Thus, the only linear relation between $\vu_1$, $\vu_2$
   and $\vu_3$ is the trivial one, as required.
  \item[(b)] These are linearly dependent, because of the
   nontrivial relation $4\vv_1-2\vv_2-\vv_3=0$.
  \item[(c)] These are linearly independent.  Indeed,
   suppose we have a relation
   $\lm_1\vw_1+\lm_2\vw_2+\lm_3\vw_3=0$.  This means that 
   \[ \lm_1\bsm 1\\1\\2\esm + 
      \lm_2\bsm 4\\5\\7\esm +
      \lm_3\bsm 1\\1\\1\esm = \bsm 0\\0\\0\esm ,
   \]
   so
   \begin{align*}
    \lm_1 + 4\lm_2 + \lm_3  &= 0 \\
    \lm_1 + 5\lm_2 + \lm_3  &= 0 \\
    2\lm_1 + 7\lm_2 + \lm_3 &= 0.
   \end{align*}
   Subtracting the first two equations gives $\lm_2=0$.
   Given this, we can subtract the last two equations to get
   $\lm_1=0$.  Feeding this back into the first equation
   gives $\lm_3=0$.  Thus, the only linear relation between
   $\vw_1$, $\vw_2$ and $\vw_3$ is the trivial one, as
   required.  

   This can also be done by matrix methods.  Let $A$ be the
   matrix whose columns are $\vw_1$, $\vw_2$ and $\vw_3$, so
   \[ A = \bsm 1 & 4 & 1 \\ 1 & 5 & 1 \\ 2 & 7 & 1 \esm. \]
   Then $\det(A)=-1\neq 0$, and if we row-reduce eihter $A$
   or $A^T$ then we get the identity.  Any of these facts
   implies that $\vw_1$, $\vw_2$ and $\vw_3$ are linearly
   independent, as you should remember from SOM201.
 \end{itemize}
\end{solution}

\begin{exercise}\label{ex-various-polys}
 Consider the list $\CV=1,x,(1+x)^2,1+x^2$ of elements of
 $\R[x]_{\leq 2}$.
 \begin{itemize}\renewcommand{\itemsep}{1ex}
  \item[(a)] Simplify $\mu_{\CV}([0,1,1,-1]^T)$.
  \item[(b)] Find $\vlm\in\R^4$ such that
   $\mu_\CV(\vlm)=x^2$.
  \item[(c)] Find $\vlm\in\R^4$ such that $\vlm\neq 0$ but
   $\mu_\CV(\vlm)=0$ (showing that $\CV$ is linearly
   dependent).
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] 
   \begin{align*}
    \mu_\CV([0,1,1,-1]^T)
     &= 0.1 + 1.x + 1.(1+x)^2 - 1.(1+x^2) \\
     &= x + 1 + 2x + x^2 - 1 - x^2 = 3x.
   \end{align*}
  \item[(b)] Here it is simplest to just observe that 
   \[ x^2 =
       -1 + (1+x^2) = (-1). 1 + 0.x + 0.(1+x)^2 + 1.(1+x^2) = 
       \mu_\CV([-1,0,0,1]^T).
   \]
   For a more laborious but systematic approach, we have
   \begin{align*}
    \mu_\CV(\vlm)
     &= \lm_1.1 + \lm_2.x + \lm_3.(1+2x+x^2) + \lm_4.(1+x^2) \\
     &= (\lm_1+\lm_3+\lm_4) + (\lm_2+2\lm_3)x + (\lm_3+\lm_4)x^2.
   \end{align*}
   We want this to equal $x^2$, so we must have
   \begin{align*}
    \lm_1 + \lm_3 + \lm_4 &= 0 \\
    \lm_2 + 2\lm_3 &= 0 \\
    \lm_3 + \lm_4 &= 1.
   \end{align*}
   These equations can be solved to give $\lm_1=-1$ and
   $\lm_2=-2\lm_3$ and $\lm_4=1-\lm_3$ (where $\lm_3$ can be
   anything).  It is simplest to take $\lm_3=0$, so
   $\lm_1=-1$ and $\lm_2=0$ and $\lm_4=1$, so
   $\vlm=([-1,0,0,1]^T)$.  
  \item[(c)] Again it is easiest to just observe that
   $(1+x)^2=(1+x^2)+2x$, so
   $0.1 + 2.x - 1.(1+x)^2 + 1.(1+x^2)=0$, so
   $\mu_\CV([0,2,-1,1]^T)=0$.  
   For a more laborious but systematic approach, recall that
   \[ \mu_\CV(\vlm) =
      (\lm_1+\lm_3+\lm_4) + (\lm_2+2\lm_3)x + (\lm_3+\lm_4)x^2.
   \]
   We want this to equal $0$, so we must have
   \begin{align*}
    \lm_1 + \lm_3 + \lm_4 &= 0 \\
    \lm_2 + 2\lm_3 &= 0 \\
    \lm_3 + \lm_4 &= 0.
   \end{align*}
   These equations can be solved to give $\lm_1=0$ and
   $\lm_3=-\lm_4$ and $\lm_2=-2\lm_3=2\lm_4$, so
   $\vlm=\lm_4.[0,2,-1,1]^T$.  Here $\lm_4$ can be anything,
   but it is simplest to take $\lm_4=1$ to get
   $\vlm=[0,2,-1,1]^T$. 
 \end{itemize}
\end{solution}

\begin{exercise}\label{ex-check-span-matrices}
 Which of the following lists of matrices spans $M_2\R$?
 \begin{itemize}\renewcommand{\itemsep}{1ex}
  \item[(a)]
   $\CA=\bsm 0&1\\2&3\esm,
        \bsm 0&2\\1&3\esm,
        \bsm 0&1\\3&2\esm,
        \bsm 0&3\\2&1\esm$
  \item[(b)]
   $\CB=\bsm 1&1\\1&0\esm,
        \bsm 0&1\\1&1\esm,
        \bsm 1&0\\0&1\esm,
        \bsm 1&0\\0&-1\esm$
  \item[(c)]
   $\CC=\bsm 1&0\\0&0\esm,
        \bsm 1&1\\0&0\esm,
        \bsm 1&1\\1&0\esm,
        \bsm 1&1\\1&1\esm$
  \item[(d)]
   $\CD=\bsm 463&859\\265&-463\esm,
        \bsm 937&724\\195&-937\esm,
        \bsm 431&736\\428&-431\esm,
        \bsm 777&152\\522&-777\esm$
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] As the matrices in $\CA$ all have $0$ in the top
   left corner, the same will be true of any matrix in
   $\spn(\CA)$.  (The formula is
   \[ \mu_\CA(\vlm) = 
       \lm_1 \bsm 0&1\\2&3\esm +
       \lm_2 \bsm 0&2\\1&3\esm +
       \lm_3 \bsm 0&1\\3&2\esm +
       \lm_4 \bsm 0&3\\2&1\esm =
       \bsm 0 & \lm_1 + 2\lm_2 + \lm_3 + 3\lm_4 \\
            2\lm_1 + \lm_2 + 3\lm_3 + 2\lm_4 &
            3\lm_1 + 3\lm_2 + 2\lm_3 + \lm_4 \esm,
   \]
   but you should be able to follow the argument without
   needing the formula.)  In particular, the identity
   matrix cannot lie in $\spn(\CA)$, because it
   does not have $0$ in the top left corner.  Thus
   $\spn(\CA)\neq M_2\R$.
  \item[(b)] As the matrices in $\CB$ are symmetric, the
   same will be true of any matrix in $\spn(\CB)$.  (The formula is
   \[ \mu_\CB(\vlm) = 
       \lm_1 \bsm 1&1\\1&0\esm +
       \lm_2 \bsm 0&1\\1&1\esm +
       \lm_3 \bsm 1&0\\0&1\esm +
       \lm_4 \bsm 1&0\\0&-1\esm =
       \bsm \lm_1 +\lm_3+\lm_4 & \lm_1+\lm_2  \\
            \lm_1+\lm_2 & \lm_2+\lm_3-\lm_4 \esm,
   \]
   but you should be able to follow the argument without
   needing the formula.)  In particular, the matrix
   $\bsm 0&1\\0&0\esm$ cannot lie in $\spn(\CB)$, because it
   is not symmetric.  Thus $\spn(\CB)\neq M_2\R$.
  \item[(c)] The list $\CC$ spans $M_2\R$.  To see this,
   consider a matrix $A=\bsm a&b\\ c&d\esm$.  We have 
   \[ \mu_\CC(\vlm)=
       \lm_1 \bsm 1&0\\0&0\esm +
       \lm_2 \bsm 1&1\\0&0\esm +
       \lm_3 \bsm 1&1\\1&0\esm +
       \lm_4 \bsm 1&1\\1&1\esm = 
       \bsm \lm_1+\lm_2+\lm_3+\lm_4 & \lm_2+\lm_3+\lm_4 \\
            \lm_3+\lm_4 & \lm_4 \esm.
   \]
   We want this to equal $\bsm a&b \\ c&d\esm$, so we must
   have
   \begin{align*}
    \lm_1 + \lm_2 + \lm_3 + \lm_4 &= a \\
            \lm_2 + \lm_3 + \lm_4 &= b \\
                    \lm_3 + \lm_4 &= c \\
                            \lm_4 &= d
   \end{align*}
   These equations have the (unique) solution $\lm_1=a-b$,
   $\lm_2=b-c$, $\lm_3=c-d$ and $\lm_4=d$.  In conclusion,
   we have
   \[ \mu_{\CC}([a-b, b-c, c-d, d]^T) = A,  \]
   showing that $A\in\spn(\CC)$.  This works for any matrix
   $A$, so $M_2\R=\spn(\CC)$.
  \item[(d)] As all the matrices in $\CD$ have trace zero,
   the same will be true of any matrix in $\spn(\CD)$.  In
   particular, the identity matrix cannot lie in
   $\spn(\CD)$, because it does not have trace zero.  Thus,
   $\spn(\CD)\neq M_2\R$.
 \end{itemize}
\end{solution}

\begin{exercise}\label{ex-prove-rk-spans}
 Put $r_k(x)=(x+k)^2$.  Prove that the list
 $\CR=r_0,r_1,r_2$ spans $\R[x]_{\leq 2}$.
\end{exercise}
\begin{solution}
 Given $\vlm=[\lm_0,\lm_1,\lm_2]^T\in\R^3$, we have 
 \begin{align*}
  \mu_\CR(\vlm)(x)
   &= \lm_0x^2 + \lm_1(x+1)^2 + \lm_2(x+2)^2 
    = \lm_0x^2 + \lm_1(x^2+2x+1) + \lm_2(x^2+4x+4) \\
   &= (\lm_0+\lm_1+\lm_2)x^2 + (2\lm_1+4\lm_2)x + (\lm_1+4\lm_2)
 \end{align*}
 Suppose we have a quadratic polynomial $q(x)=ax^2+bx+c$,
 and we want to have $\mu_\CR(\vlm)=q$.  We must then have
 \begin{align*}
  \lm_0+\lm_1+\lm_2 &= a \\
  2\lm_1 + 4\lm_2 &= b \\
  \lm_1 + 4\lm_2 &= c.
 \end{align*}
 Subtracting the last two equations gives $\lm_1=b-c$, and
 we can put this into the last equation to give
 $\lm_2=c/2-b/4$.  We then put these two values back into
 the first equation to give $\lm_0=a-3b/4+c/2$.  The
 conclusion is that
 \[ \mu\left(\bsm a-3b/4+c/2 \\ b-c \\ c/2-b/4\esm\right) = q,
 \]
 showing that $q\in\spn(\CR)$.  As this works for any
 quadratic polynomial $q$, we have $\spn(\CR)=\R[x]_{\leq 2}$.
\end{solution}

\begin{exercise}\label{ex-three-evals}
 Suppose we have real numbers $a,b,c\in\R$ and functions
 $f,g,h\in C(\R)$ such that 
 \begin{align*}
  f(a) &= 1 && g(a) &= 0 && h(a) &= 0 \\
  f(b) &= 0 && g(b) &= 1 && h(b) &= 0 \\
  f(c) &= 0 && g(c) &= 0 && h(c) &= 1
 \end{align*}
 Prove that $f$, $g$ and $h$ are linearly independent.
\end{exercise}
\begin{solution}
 Suppose we have a linear relation $\lm f+\mu g+\nu h=0$.
 Note that the symbol $0$ on the right hand side means the
 zero function, which takes the value $0$ for all $x$.  In
 particular, it takes the value $0$ at $x=a$, so we have
 \[ \lm f(a) + \mu g(a) + \nu h(a) = 0. \]
 As $f(a)=1$ and $g(a)=h(a)=0$, this simplifies to $\lm=0$.
 Similarly, we have
 \begin{align*}
  \lm f(b) + \mu g(b) + \nu h(b) &= 0 \\
  \lm f(c) + \mu g(c) + \nu h(c) &= 0,
 \end{align*}
 and these simplify to give $\mu=\nu=0$.  Thus, the only
 linear relation between $f$, $g$ and $h$ is the trivial
 one, so they are linearly independent.
\end{solution}

\begin{exercise}\label{ex-exp-wronskian}
 Put $f_k(x)=e^{kx}$.  Calculate $W(f_1,f_2,f_3)$.  Are
 $f_1$, $f_2$ and $f_3$ linearly independent?
\end{exercise}
\begin{solution}
 We have $f'_k(x)=ke^{kx}$ and $f''_k(x)=k^2e^{kx}$, so
 \begin{align*}
  W(f_1,f_2,f_3)(x)
   &= \det\bbm e^x &   e^{2x} &   e^{3x} \\
               e^x & 2 e^{2x} & 3 e^{3x} \\
               e^x & 4 e^{2x} & 9 e^{3x} \ebm  
    = e^x e^{2x} e^{3x} \det \bbm 1&1&1 \\ 1&2&3 \\ 1&4&9 \ebm \\
   &= e^{6x}\left(
         \det\bbm 2&3\\4&9 \ebm 
       - \det\bbm 1&3\\1&9 \ebm
       + \det\bbm 1&2\\1&4 \ebm 
      \right) 
    = e^{6x} (6 - 6 + 2) = 2 e^{6x}.
 \end{align*}
 This is not the zero function, so $f_1$, $f_2$ and $f_3$
 are linearly independent.
\end{solution}

\begin{exercise}\label{ex-pow-wronskian}
 Find and simplify the Wronskian of the functions
 $g_0(x)=x^n$, $g_1(x)=x^{n+1}$ and $g_2(x)=x^{n+2}$.  You
 may want to use Maple for this.  If you do, you will need
 to use \verb~simplify(...)~ to get the answer in
 its simplest form.
\end{exercise}
\begin{solution}
 The Wronskian matrix is 
 \[ WM = \bbm 
     x^n           & x^{n+1}       & x^{n+2}           \\
     n x^{n-1}     & (n+1) x^n     & (n+2) x^{n+1}     \\
     n(n-1)x^{n-2} & n(n+1)x^{n-1} & (n+1)(n+2) x^n 
    \ebm 
 \]
 We can extract a factor of $x^n$ from the first row, $x^{n-1}$ from
 the second row, and $x^{n-2}$ from the third row, to get
 \[ W = \det(WM) = x^{3n-3} \det\bbm
     1      & x       & x^2           \\
     n      & (n+1) x & (n+2) x^2     \\
     n(n-1) & n(n+1)x & (n+1)(n+2) x^2 
    \ebm .
 \]
 We then extract $x$ from the second column, and $x^2$ from the third
 column, to get $W=x^{3n}\det(V)$, where 
 \[ V = \bbm
     1      & 1      & 1        \\
     n      & n+1    & n+2      \\
     n(n-1) & n(n+1) & (n+1)(n+2)  
    \ebm .
 \]

 We will expand $\det(V)$ along the top row, using the cofactors
 {\tiny \begin{align*}
  \det\bbm (n+1) & (n+2)  \\ n(n+1) & (n+1)(n+2) \ebm
   &= (n+1)^2(n+2) - n(n+1)(n+2) = (n+1)(n+2) = n^2+3n+2 \\
  \det\bbm n  & (n+2)  \\ n(n-1) & (n+1)(n+2) \ebm
   &= n(n+1)(n+2) - n(n-1)(n+2) 
    = 2n(n+2) = 2n^2+4n \\
  \det\bbm n & (n+1) \\ n(n-1) & n(n+1) \ebm
   &= n^2(n+1) - n(n-1)(n+1) 
    = n(n+1) = (n^2+n).
 \end{align*}}
 Thus
 \[ \det(V) =
   1 . (n^2+3n+2) - 1.(2n^2+4n) + 1.(n^2+n) =
   n^2+3n+2-2n^2-4n+n^2+n = 2,
 \]
 so $W=2x^{3n}$.  Alternatively, you could ask Maple:
\begin{verbatim}
  with(LinearAlgebra):

  WM := simplify(
        <<      x^n     ,      x^(n+1)     ,      x^(n+2)      >| 
         < diff(x^n,x)  , diff(x^(n+1),x)  , diff(x^(n+2),x)   >|
         < diff(x^n,x,x), diff(x^(n+1),x,x), diff(x^(n+2),x,x) >>
        );

  W := simplify(Determinant(WM));
\end{verbatim}
\end{solution}

\begin{exercise}\label{ex-surj-misc-i}
 Define a linear map $\phi\:M_3\R\to\R[x]_{\leq 4}$ by
 \[ \phi(A) = [1,x,x^2] A [1,x,x^2]^T. \]
 Show that $\phi$ is surjective, and find a basis for its kernel.
\end{exercise}
\begin{solution}
 Consider a matrix
 \[ A = \bpm a_1&a_2&a_3\\ a_4&a_5&a_6 \\ a_7&a_8&a_9 \epm \]
 Explicitly, we have
 \begin{align*}
  \phi(A)
  &= \bpm 1 & x & x^2\epm
     \bpm a_1&a_2&a_3\\ a_4&a_5&a_6 \\ a_7&a_8&a_9 \epm
     \bpm 1 \\ x \\ x^2\epm \\
  &= \bpm 1 & x & x^2 \epm
     \bpm a_1 + a_2x + a_3 x^2 \\
          a_4 + a_5x + a_6 x^2 \\
          a_7 + a_8x + a_9 x^2 \epm \\
  &= a_1 + (a_2+a_4)x + (a_3+a_5+a_7)x^2 + (a_6+a_8)x^3 + a_9x^4
 \end{align*}
 In particular, given any element $f(x)=b_0+b_1x+b_2x^2+b_3x^3+b_4x^4$ in
 $\R[x]_{\leq 4}$, we can take
 \[ a_1=b_0,\; a_2=b_1,\; a_3=b_2,\; a_6=b_3,\; a_9=b_4,\;
    a_4=a_5=a_7=a_8=0
 \]
 and we then have $\phi(A)=f$.  More explicitly:
 \[ \phi\bsm b_0 & b_1 & b_2 \\ 0 & 0 & b_3 \\ 0 & 0 & b_4 \esm 
    = \bpm 1 & x & x^2\epm 
      \bsm b_0 & b_1 & b_2 \\ 0 & 0 & b_3 \\ 0 & 0 & b_4 \esm 
      \bpm 1 \\ x \\ x^2 \epm 
    = b_0 + b_1 x + b_2 x^2 + b_3 x^3 + b_4 x^4.
 \] 
 This means that $\phi$ is
 surjective.  The kernel is the set of matrices $A$ for which
 \[ a_1 = a_2+a_4 = a_3+a_5+a_7 = a_6+a_8 = a_9 = 0, \]
 or in other words, the set of matrices of the form
 {\tiny \[ A = \bsm 0 &\; & a_2 &\; & a_3 \\
             -a_2 &\; & a_5 &\; & a_6 \\
             -a_3-a_5 &\; & -a_6 &\; & 0 \esm = 
   a_2 \bpm  0&1&0\\ -1&0&0\\ 0&0&0 \epm + 
   a_3 \bpm 0&0&1\\ 0&0&0\\ -1&0&0 \epm + 
   a_5 \bpm 0&0&0\\ 0&1&0\\ -1&0&0 \epm + 
   a_6 \bpm 0&0&0\\ 0&0&1\\ 0&-1&0 \epm
 \]}
 It follows that the following matrices form a basis for
 $\ker(\phi)$:
 {\tiny \[
  \bpm 0&1&0\\ -1&0&0\\ 0&0&0 \epm \hspace{4em}
  \bpm 0&0&1\\ 0&0&0\\ -1&0&0 \epm \hspace{4em}
  \bpm 0&0&0\\ 0&1&0\\ -1&0&0 \epm \hspace{4em}
  \bpm 0&0&0\\ 0&0&1\\ 0&-1&0 \epm
 \]}
\end{solution}


\begin{exercise}\label{ex-surj-misc-ii}
 \begin{itemize}
  \item[(a)] Define a map $\phi\:\R[x]_{\leq 3}\to\R^2$ by
  $\phi(f)=[f(0),f(1)]^T$.  Show that this is surjective, and that
  the kernel is spanned by $x^2-x$ and $x^3-x^2$.
  \item[(b)] Define a map $\psi\:\R[x]_{\leq 2}\to\R^4$ by
  $\psi(f)=[f(0),f(1),f(2),f(3)]^T$.  Show that this is injective,
  and that the image is the space
  \[ V = \{[u_0,u_1,u_2,u_3]^T\in\R^4\st u_0-3u_1+3u_2-u_3=0\}. \]
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] Given any vector $\vv=\bsm a\\ b\esm\in\R^2$,
   we can define $f(x)=a+(b-a)x$; then $f\in\R[x]_{\leq 3}$
   and $f(0)=a$ and $f(1)=b$, so $\phi(f)=\vv$.  This shows
   that $\phi$ is surjective.

   Now consider a polynomial $f(x)=ax^3+bx^2+cx+d$.
   We have $\phi(f)=0$ iff $f(1)=0=f(0)$ iff $a+b+c+d=0=d$
   iff $c=-a-b$ and $d=0$.  If this holds then 
   \[ f(x) = ax^3 + bx^2 + (-a-b)x = a(x^3-x) + b(x^2-x) 
       = a(x^3-x^2) + (a+b)(x^2-x).
   \]
   In other words, if we put $p(x)=x^3-x^2$ and
   $q(x)=x^2-x$, then $\phi(p)=\phi(q)=0$ and
   $f=a\,p+(a+b)\,q\in\spn(p,q)$.  This shows that $p$ and
   $q$ span $\ker(\phi)$, and they are clearly linearly
   independent, so they give a basis for $\ker(\phi)$.
  \item[(b)] If $\psi(f)=0$ then we have
   $f(0)=f(1)=f(2)=f(3)=0$, so $f(x)$ has at least four
   different roots.  As $f$ is a polynomial of degree at
   most three, this is impossible, unless $f=0$.  To be more
   explicit, suppose that $f(x)=ax^3+bx^2+cx+d$ and
   $f(0)=f(1)=f(2)=f(3)=0$.  This means that
   \begin{align*}
    d &= 0 \\
    a+b+c+d &= 0 \\
    8a+4b+2c+d &= 0 \\
    27a+9b+3c+d &= 0,
   \end{align*}
   and these equations can be solved in the standard way to
   show that $a=b=c=d=0$.
 \end{itemize}
\end{solution}

\begin{exercise}\label{ex-trace-rep}
 Given vectors $[p,q]^T,[r,s]^T\in\R^2$, we can define a linear
 map $\phi\:M_2\R\xra{}\R$ by 
 \[ \phi(A) = \bpm p & q \epm A \bpm r \\ s\epm. \]
 Show that $p$, $q$, $r$ and $s$ \textbf{cannot} be chosen
 so that $\phi(A)=\trace(A)$ for all $A\in M_2\R$.  
\end{exercise}
\begin{solution}
 First consider the matrices 
 \[ E_1 = \bpm 1&0\\0&0 \epm \hspace{3em}
    E_2 = \bpm 0&1\\0&0 \epm \hspace{3em}
    E_3 = \bpm 0&0\\1&0 \epm \hspace{3em}
    E_4 = \bpm 0&0\\0&1 \epm.
 \]
 We have
 \begin{align*}
  \phi(E_1) &= \bpm p&q\epm\bpm r \\ 0\epm = pr \\
  \phi(E_2) &= \bpm p&q\epm\bpm s \\ 0\epm = ps \\
  \phi(E_3) &= \bpm p&q\epm\bpm 0 \\ r\epm = qr \\
  \phi(E_4) &= \bpm p&q\epm\bpm 0 \\ s\epm = qs.
 \end{align*}
 On the other hand, we have $\trace(E_1)=\trace(E_4)=1$ and
 $\trace(E_2)=\trace(E_3)=0$.  Suppose for a contradiction
 that we have $\phi(A)=\trace(A)$.  By taking $A=E_i$ for
 $i=1,2,3,4$ we get $pr=1$ and $ps=0$ and $qr=0$ and
 $qs=1$.  As $pr=qs=1$ we see that all of $p$, $q$, $r$ and
 $s$ must be nonzero.  This conflicts with the equations
 $ps=0=qr$, so we have the required contradiction.
\end{solution}

\closegraphsfile
\end{document}



%%% Local Variables:
%%% compile-command: "do_both 04"
%%% End:
